{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8_3_Normalization_In_Training.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP8PKkZWapWdSJmiC71mtul"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ALjOkEvMGzDh","colab_type":"text"},"source":["The normalization is a tip not only for preprocessing data but also for improving training and inference. This tutorial shows several normalization methods used in training.\n","\n","Reference: https://arxiv.org/abs/1803.08494\n","\n","![](https://github.com/shaohua0116/Group-Normalization-Tensorflow/raw/master/figure/gn.png)\n","\n","Tensorflow.org (2020)\n","\n","In the following, we use the x as the batch data for normalization."]},{"cell_type":"code","metadata":{"id":"yTmDbvLzx6i2","colab_type":"code","colab":{}},"source":["!pip install -q tf-nightly tensorflow-addons"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SxZ-JmH_x9YW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"d23221b6-f42c-4aee-d084-4bb966ccbdbc","executionInfo":{"status":"ok","timestamp":1583313583049,"user_tz":-480,"elapsed":1023,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["import tensorflow as tf\n","import tensorflow_addons as tfa\n","\n","print(\"Tensorflow Version: {}\".format(tf.__version__))\n","print(\"GPU {} available.\".format(\"is\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"not\"))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Tensorflow Version: 2.2.0-dev20200303\n","GPU not available.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nz7z8TpjbQVx","colab_type":"code","colab":{}},"source":["x = tf.random.normal(shape=(3, 2, 3, 2))  # (batch_size, H, W, Channel)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJpp9FGVI7U8","colab_type":"text"},"source":["# Batch Normalization"]},{"cell_type":"markdown","metadata":{"id":"skfZJ1qQTcFv","colab_type":"text"},"source":["## The Origin Batch Normalization using `tf.nn.batch_normalization`\n","\n","$$\\mu_{j} = \\frac{1}{m}\\sum^{m}_{i=1} x_{ij}$$\n","$$\\sigma^{2}_{j}=\\frac{1}{m}\\sum^{m}_{i=1}(x_{ij}-\\mu_{j})^2$$\n","$$\\hat{x_{ij}}=\\frac{x_{ij}-\\mu_{j}}{\\sqrt{\\sigma^2_j + \\epsilon}}$$"]},{"cell_type":"code","metadata":{"id":"dXJd6uf7I9wJ","colab_type":"code","outputId":"843ac587-1d20-4168-e87e-fdccb983baad","executionInfo":{"status":"ok","timestamp":1583314586711,"user_tz":-480,"elapsed":806,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["means, vars = tf.nn.moments(x, axes=[0, 1, 2], keepdims=True)\n","bn_scratch = (x - means) / tf.sqrt(vars + 1e-3)\n","\n","# through tf.nn.batch_normalization\n","b_ori = tf.nn.batch_normalization(x, means, vars, offset=None, scale=None, variance_epsilon=1e-3)\n","\n","print(\"mean shape: {}, vars shape: {}\".format(means.shape, vars.shape))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["mean shape: (1, 1, 1, 2), vars shape: (1, 1, 1, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CGYLPsq-5Yvc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"acfea67c-91ff-48ab-f923-12cc58e1a1f7","executionInfo":{"status":"ok","timestamp":1583314487228,"user_tz":-480,"elapsed":970,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["bn_scratch[0, ...], b_ori[0, ...]"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n"," array([[[ 1.0182704 ,  0.5101875 ],\n","         [ 0.6017975 , -1.3783205 ],\n","         [-0.3881208 ,  1.2821097 ]],\n"," \n","        [[ 1.1666497 ,  1.3473433 ],\n","         [ 0.53064954,  0.19922358],\n","         [-1.4957787 ,  0.26144832]]], dtype=float32)>,\n"," <tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n"," array([[[ 1.0182704 ,  0.5101875 ],\n","         [ 0.6017976 , -1.3783205 ],\n","         [-0.38812083,  1.2821096 ]],\n"," \n","        [[ 1.1666497 ,  1.3473433 ],\n","         [ 0.53064954,  0.19922358],\n","         [-1.4957788 ,  0.26144832]]], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"oBBpaJ--TqSr","colab_type":"text"},"source":["## The Advanced BN using `tf.keras.layers.BatchNormalization`\n","\n","The `tf.keras.layers.BatchNormalization` is a higher API for the batch normalization. However, this API is slightly different from the lower API `tf.nn.batch_normalization`. \n","\n","Batch normalization consists of two main calculations, one is the standard normalization `x_hat = (x - mean) / sqrt(x + epsilon)`, and the second is the **de-normalization** `y = gamma * x_hat + beta`. The lower-level API implements the first calculation and the higher-level API implements the whole calculation.\n","\n","This higher-level API mainly includes the following parameters.\n","* `mean`, `variance`: the mean and variance of the batch data\n","* `gamma`, `beta`: are **trainable** based on the gradients\n","* `running_mean` and `running_var`: collected the mean and the variance from each batch and averaged them\n","\n","In training,\n","* The first step is to calculate the mean and variance and apply to the data, `x_hat = (x - mean) / sqrt(variance + epsilon)`.\n","* The second step is to multiply the gamma and add the beta up to the x_hat, `out = x_hat * gamma + beta`. (We expect the gamma and the beta can be learned from the data.)\n","* The third step is to add the current mean and variance to the `running_mean` and `running_variance`, `running_mean = momentum * running_mean + (1 - momentum) * mean_batch` and `running_variance = momentum * running_variance + (1 - momentum) * variance_batch`.\n","\n","From the above calculation, we can expect the running_mean and the running_variance are more batch sizes more persuasive.\n","\n","In inference,\n","The data feed into the model is not in a unit of the batch, so you can't calculate the mean and the variance. Instead, you would use the `running_mean` and `running_variance`. \n","The output would become, `x_hat = (x - running_mean) / sqrt(running_variance + epsilon)` and the output `out = x_hat * gamma + beta`.\n","\n","However, if you are going to use a higher API without the training, the mean and variance can't be calculated, so you have to use the `running_mean` and `running_variance`. But, at this moment there is also no historical `running_mean` and `running_variance`. The value is at their defaults, `0` and `1`."]},{"cell_type":"code","metadata":{"id":"En3i3G5HTp8C","colab_type":"code","colab":{}},"source":["# scale: gamma, center: beta\n","# momentum: only available in the training mode\n","b = tf.keras.layers.BatchNormalization(\n","    axis=-2, trainable=False, scale=False, center=False, epsilon=0.001)(x)\n","\n","running_mean, running_vars = 0., 1.\n","bnorm_scratch = (x - running_mean) / tf.sqrt(running_vars + 1e-3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3SYquM3U5IJh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"9414b233-1edc-4b27-e40d-3654f37a1a52","executionInfo":{"status":"ok","timestamp":1583314417167,"user_tz":-480,"elapsed":799,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["bnorm_scratch[0, ...], b[0, ...]"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n"," array([[[ 1.3049238 ,  0.6310653 ],\n","         [ 0.92743635, -1.4323952 ],\n","         [ 0.0301825 ,  1.4744987 ]],\n"," \n","        [[ 1.4394137 ,  1.5457758 ],\n","         [ 0.86294836,  0.2912935 ],\n","         [-0.9737895 ,  0.3592828 ]]], dtype=float32)>,\n"," <tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n"," array([[[ 1.3049238 ,  0.6310653 ],\n","         [ 0.9274363 , -1.4323952 ],\n","         [ 0.0301825 ,  1.4744987 ]],\n"," \n","        [[ 1.4394135 ,  1.5457757 ],\n","         [ 0.86294836,  0.2912935 ],\n","         [-0.9737895 ,  0.3592828 ]]], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"Hjp3A5G5Gh6z","colab_type":"text"},"source":["# Layer Normalization\n","\n","In Tensorflow, by default, the layer normalization is only to normalize the last dimension of tensors (the channel), that is, the axis is set to [-1]. However, the layer normalization is to normalize the whole channels within the whole input dimensions, that is to set the axis to [1,2,3].\n","\n","$$\\mu_{i} = \\frac{1}{m}\\sum^{m}_{j=1} x_{ij}$$\n","$$\\sigma^{2}_{i}=\\frac{1}{m}\\sum^{m}_{j=1}(x_{ij}-\\mu_{i})^2$$\n","$$\\hat{x_{ij}}=\\frac{x_{ij}-\\mu_{i}}{\\sqrt{\\sigma^2_i + \\epsilon}}$$\n","\n"]},{"cell_type":"code","metadata":{"id":"WTLNPvUflfRQ","colab_type":"code","outputId":"ac5c86de-eb52-493b-da44-7ca4f74812bd","executionInfo":{"status":"ok","timestamp":1583316228927,"user_tz":-480,"elapsed":787,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["l = tf.keras.layers.LayerNormalization(axis=[1,2,3])(x)\n","\n","means, vars = tf.nn.moments(x, axes=[1, 2, 3], keepdims=True)\n","lnorm_scratch = (x - means) / tf.sqrt(vars + 1e-3)\n","\n","print(\"mean shape: {}, vars shape: {}\".format(means.shape, vars.shape))"],"execution_count":76,"outputs":[{"output_type":"stream","text":["mean shape: (3, 1, 1, 1), vars shape: (3, 1, 1, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"37g-ihwB47FV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"39c4df03-3b0d-49f4-ff9a-8eb0bdf45eec","executionInfo":{"status":"ok","timestamp":1583316229887,"user_tz":-480,"elapsed":568,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["lnorm_scratch[1, ...], l[1, ...]"],"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n"," array([[[ 0.16816942, -0.91264147],\n","         [ 2.6653469 ,  0.03617269],\n","         [ 0.57133687, -1.2314578 ]],\n"," \n","        [[-0.6139934 , -0.82229173],\n","         [ 0.0084926 , -0.7341101 ],\n","         [ 0.11144427,  0.75353146]]], dtype=float32)>,\n"," <tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n"," array([[[ 0.16816941, -0.91264135],\n","         [ 2.6653466 ,  0.0361727 ],\n","         [ 0.57133687, -1.2314576 ]],\n"," \n","        [[-0.6139933 , -0.8222917 ],\n","         [ 0.00849261, -0.73411   ],\n","         [ 0.11144428,  0.75353146]]], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"markdown","metadata":{"id":"VCsah78U1oma","colab_type":"text"},"source":["# Instance Normalization"]},{"cell_type":"code","metadata":{"id":"2mIQc_ek4h9Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f6bc4bc5-7406-4634-a203-848d602b8c93","executionInfo":{"status":"ok","timestamp":1583314557283,"user_tz":-480,"elapsed":823,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["inorm = tfa.layers.InstanceNormalization(center=False, scale=False, epsilon=1e-6)(x)\n","\n","means, vars = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n","inorm_scratch = (x - means) / tf.sqrt(vars + 1e-6)\n","\n","print(\"mean shape: {}, vars shape: {}\".format(means.shape, vars.shape))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["mean shape: (3, 1, 1, 2), vars shape: (3, 1, 1, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"71yVSjnF2LG3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"ce08d824-43d7-4a48-c8ee-f263cfe9918d","executionInfo":{"status":"ok","timestamp":1583314562387,"user_tz":-480,"elapsed":850,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["inorm[1, ...], inorm_scratch[1, :]"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n"," array([[[-0.30603546, -0.6347712 ],\n","         [ 2.1050472 ,  0.77404225],\n","         [ 0.08323207, -1.1081547 ]],\n"," \n","        [[-1.0612316 , -0.5006187 ],\n","         [-0.4602071 , -0.36968517],\n","         [-0.3608049 ,  1.8391874 ]]], dtype=float32)>,\n"," <tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n"," array([[[-0.30603546, -0.6347713 ],\n","         [ 2.1050472 ,  0.7740423 ],\n","         [ 0.08323209, -1.1081547 ]],\n"," \n","        [[-1.0612317 , -0.5006187 ],\n","         [-0.4602071 , -0.36968526],\n","         [-0.3608049 ,  1.8391874 ]]], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"OJk-c4tN58-D","colab_type":"text"},"source":["# Group Normalization"]},{"cell_type":"markdown","metadata":{"id":"mp7zAgxE73bL","colab_type":"text"},"source":["Group Normalization divides the channels into groups and computes within each group the mean and the variance for normalization. \n","\n","If the parameter `groups` is set to 1, it is identical to the `layer normalization`. \n","\n","If the parameters `groups` is set to the input dimension (number of groups is equal to numbers of channels, e.g. image data shape [batch_size, H, W, C], the number of groups is 2(H, and W)), it is identical to `instance normalization`."]},{"cell_type":"markdown","metadata":{"id":"XU0kUWik-8Iy","colab_type":"text"},"source":["## Two Input Dimensions (`groups=2`)"]},{"cell_type":"code","metadata":{"id":"gohIzBBB6Afv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"aa9cd40f-803b-489d-fc3f-d728b3d06098","executionInfo":{"status":"ok","timestamp":1583316088232,"user_tz":-480,"elapsed":1071,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["gnorm = tfa.layers.GroupNormalization(groups=2, epsilon=1e-6, center=False, scale=False)(x)\n","\n","means, vars = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n","gnorm_scratch = (x - means) / tf.sqrt(vars + 1e-6)\n","\n","print(\"mean shape: {}, vars shape: {}\".format(means.shape, vars.shape))"],"execution_count":69,"outputs":[{"output_type":"stream","text":["mean shape: (3, 1, 1, 2), vars shape: (3, 1, 1, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lbbGF8ux6MWl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"a89cb15b-8323-4753-d895-867f36df71dd","executionInfo":{"status":"ok","timestamp":1583316088779,"user_tz":-480,"elapsed":598,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["gnorm[1, ...], gnorm_scratch[1, ...]"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n"," array([[[-0.30603546, -0.6347712 ],\n","         [ 2.1050472 ,  0.77404225],\n","         [ 0.08323207, -1.1081547 ]],\n"," \n","        [[-1.0612316 , -0.5006187 ],\n","         [-0.4602071 , -0.36968517],\n","         [-0.3608049 ,  1.8391874 ]]], dtype=float32)>,\n"," <tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n"," array([[[-0.30603546, -0.6347713 ],\n","         [ 2.1050472 ,  0.7740423 ],\n","         [ 0.08323209, -1.1081547 ]],\n"," \n","        [[-1.0612317 , -0.5006187 ],\n","         [-0.4602071 , -0.36968526],\n","         [-0.3608049 ,  1.8391874 ]]], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"AymOnXca-32b","colab_type":"text"},"source":["## Three Input Dimensions (`groups=1`)"]},{"cell_type":"code","metadata":{"id":"M0n9LxhS9pof","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a8f18944-089e-48fc-a2f4-901118c16916","executionInfo":{"status":"ok","timestamp":1583316093035,"user_tz":-480,"elapsed":766,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["gnorm = tfa.layers.GroupNormalization(groups=1, epsilon=1e-6, center=False, scale=False)(x)\n","\n","means, vars = tf.nn.moments(x, axes=[1, 2, 3], keepdims=True)\n","gnorm_scratch = (x - means) / tf.sqrt(vars + 1e-6)\n","\n","print(\"mean shape: {}, vars shape: {}\".format(means.shape, vars.shape))"],"execution_count":71,"outputs":[{"output_type":"stream","text":["mean shape: (3, 1, 1, 1), vars shape: (3, 1, 1, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pa_kUcZs-uFx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"50e332bc-1e16-4c93-d26d-f60dae86aad5","executionInfo":{"status":"ok","timestamp":1583316093278,"user_tz":-480,"elapsed":708,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["gnorm[1, ...], gnorm_scratch[1, ...]"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n"," array([[[ 0.16829652, -0.9133313 ],\n","         [ 2.6673615 ,  0.03620002],\n","         [ 0.57176876, -1.2323886 ]],\n"," \n","        [[-0.6144575 , -0.8229133 ],\n","         [ 0.008499  , -0.73466504],\n","         [ 0.1115285 ,  0.75410104]]], dtype=float32)>,\n"," <tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n"," array([[[ 0.16829653, -0.9133313 ],\n","         [ 2.6673615 ,  0.03620003],\n","         [ 0.57176876, -1.2323886 ]],\n"," \n","        [[-0.6144575 , -0.8229133 ],\n","         [ 0.00849902, -0.734665  ],\n","         [ 0.11152851,  0.75410104]]], dtype=float32)>)"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"s_NYkKtq-wqw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}