{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1050,
     "status": "ok",
     "timestamp": 1557756273840,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "nE4wVCWph1FK",
    "outputId": "ab0395df-541f-41af-cec9-d571c98a82ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/deep_learning_from_scratch\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd /content/gdrive/My\\ Drive/deep_learning_from_scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-m6q5QNVvxA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QwXlP68Tc740"
   },
   "outputs": [],
   "source": [
    "from common.seq2seq import Encoder, Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbXe51gr1nau"
   },
   "outputs": [],
   "source": [
    "from common.time_layers import TimeEmbedding, TimeSoftmaxWithLoss, TimeAffine, TimeLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HBFqWQcEKXM"
   },
   "outputs": [],
   "source": [
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eQPIHOI2l9IM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5veBs_QcmURu"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8LvnvUwLU3Gx"
   },
   "source": [
    "# Improve Encoder and Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CtKkB43jU6qB"
   },
   "source": [
    "## Context Vector\n",
    "\n",
    "Train a weight (or vector) to multiply the hidden state vectors over the whole time from encoder (a.k.a. `hs`). This represents paying a attention to the important time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1142,
     "status": "ok",
     "timestamp": 1557756277132,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "sHPGT5wDU2oZ",
    "outputId": "8d65b218-2e46-484c-e6c4-2c146348b54b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights shape: (5, 4)\n",
      "time point * attention weights: (5, 4)\n",
      "context vector shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "\n",
    "# attention weights representing the weights for each time point\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(-1, 1).repeat(4, axis=1)\n",
    "print(\"attention weights shape: {}\".format(ar.shape))\n",
    "\n",
    "t = hs * ar\n",
    "print(\"time point * attention weights: {}\".format(t.shape))\n",
    "\n",
    "# context vector\n",
    "c = np.sum(t, axis=0)\n",
    "print(\"context vector shape: {}\".format(c.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JC6_Yo7UbnnC"
   },
   "source": [
    "In batch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1512,
     "status": "ok",
     "timestamp": 1557756277831,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "Wo7Wwg8SY0Dm",
    "outputId": "bd7ce07a-d7a9-4e57-80f7-6835d5651365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weight shape: (10, 5, 4)\n",
      "time point * attention weight: shape is (10, 5, 4)\n",
      "context vector shape in batch: (10, 4)\n"
     ]
    }
   ],
   "source": [
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "\n",
    "# attention weights\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "print(\"attention weight shape: {}\".format(ar.shape))\n",
    "\n",
    "t = hs * ar\n",
    "print(\"time point * attention weight: shape is {}\".format(t.shape))\n",
    "\n",
    "# context vector in batch data\n",
    "c = np.sum(t, axis=1)\n",
    "print(\"context vector shape in batch: {}\".format(c.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DZshBSYucxJl"
   },
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "  def __init__(self):\n",
    "    self.params, self.grads = [], []\n",
    "    self.cache = None\n",
    "  \n",
    "  def forward(self, hs, a):\n",
    "    N, T, H = hs.shape\n",
    "    ar = a.reshape(N,T,1).repeat(H, axis=2)\n",
    "    t = hs * ar\n",
    "    c = np.sum(t, axis=1)  # shape = N x H\n",
    "    self.cache = (hs, ar)\n",
    "    return c\n",
    "  \n",
    "  def backward(self, dc):\n",
    "    \"\"\"\n",
    "    dc = N x H\n",
    "    \"\"\"\n",
    "    \n",
    "    hs, ar = self.cache\n",
    "    N, T, H = hs.shape\n",
    "    \n",
    "    dt = dc.reshape(N, 1, H).repeat(T, axis=1)  # backpropagation of sum ops\n",
    "    dar = dt * hs   # shape = (N, T, H)\n",
    "    dhs = dt * ar\n",
    "    da = np.sum(dar, axis=2)   # shape = (N, T), backpropagation of repeat ops\n",
    "    \n",
    "    return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tthk7cM7iSGI"
   },
   "source": [
    "## Learning Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2044,
     "status": "ok",
     "timestamp": 1557756278824,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "zVfA7bs0gYOV",
    "outputId": "d0e81615-7db8-4cd8-874c-1cf6c5600c7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "(10, 5)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "\n",
    "# from LSTM\n",
    "h = np.random.randn(N, H)\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "\n",
    "# similar to inner product of two vectors\n",
    "# here we want to get attention weight\n",
    "# we sum hidden values (axis=2)\n",
    "t = hs * hr\n",
    "print(t.shape)\n",
    "s = np.sum(t, axis=2)\n",
    "print(s.shape)\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)\n",
    "print(np.sum(a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ml00q-0jkaW3"
   },
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "  def __init__(self):\n",
    "    self.params, self.grads = [], []\n",
    "    self.softmax = Softmax()\n",
    "    self.cache = None\n",
    "  \n",
    "  def forward(self, hs, h):\n",
    "    N, T, H = hs.shape\n",
    "    hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "    t = hs * hr\n",
    "    s = np.sum(t, axis=2)\n",
    "    a = self.softmax.forward(s)\n",
    "    \n",
    "    self.cache = (hs, hr)\n",
    "    return a\n",
    "  \n",
    "  def backward(self, da):\n",
    "    hs, hr = self.cache\n",
    "    N, T, H = hs.shape\n",
    "    \n",
    "    ds = self.softmax.backward(da)   # (N, T)\n",
    "    dt = ds.reshape(N, T, 1).repeat(H, axis=2)  # (N, T, H)\n",
    "    dhs = dt * hr\n",
    "    dhr = dt * hs\n",
    "    dh = np.sum(dhr, axis=1)\n",
    "    \n",
    "    return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zk2bMPvxgidc"
   },
   "source": [
    "## Attention\n",
    "\n",
    "Attention layer is sequentially composed of two sub-layers, `AttentionWeight` and `WeightSum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_7iVfl3SJ2K"
   },
   "outputs": [],
   "source": [
    "class Attention:\n",
    "  def __init__(self):\n",
    "    self.params, self.grads = [], []\n",
    "    self.attention_weight_layer = AttentionWeight()\n",
    "    self.weight_sum_layer = WeightSum()\n",
    "    self.attention_weight = None\n",
    "  \n",
    "  def forward(self, hs, h):\n",
    "    \"\"\"\n",
    "    hs shape: (N, T, H)\n",
    "    h shape: (H, H)\n",
    "    \"\"\"\n",
    "    aw = self.attention_weight_layer.forward(hs, h)\n",
    "    context = self.weight_sum_layer.forward(hs, aw)\n",
    "    self.attention_weight = aw\n",
    "    return context\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "    dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "    dhs = dhs0 + dhs1\n",
    "    return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29sySqYhj9jS"
   },
   "source": [
    "## Time Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rTZS7egAh-DM"
   },
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "  def __init__(self):\n",
    "    self.params, self.grads = [], []\n",
    "    self.layers = None\n",
    "    self.attention_weights = None\n",
    "  \n",
    "  def forward(self, hs_enc, hs_dec):\n",
    "    N, T, H = hs_dec.shape\n",
    "    out = np.empty_like(hs_dec)\n",
    "    self.layers = []\n",
    "    self.attention_weights = []\n",
    "    \n",
    "    for t in range(T):\n",
    "      layer = Attention()\n",
    "      out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
    "      self.layers.append(layer)\n",
    "      self.attention_weights.append(layer.attention_weight)\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    N, T, H = dout.shape\n",
    "    dhs_enc = 0\n",
    "    dhs_dec = np.empty_like(dout)\n",
    "    \n",
    "    for t in range(T):\n",
    "      layer = self.layers[t]\n",
    "      dhs, dh = layer.backward(dout[:,t,:])\n",
    "      dhs_enc += dhs\n",
    "      dhs_dec[:, t, :] = dh\n",
    "      \n",
    "    return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QEkiWbwcBLM1"
   },
   "source": [
    "# Seq2Seq with Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjbaHSKkB67y"
   },
   "source": [
    "## AttentionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ej8cXnb3Uzdv"
   },
   "outputs": [],
   "source": [
    "class AttentionEncoder(Encoder):\n",
    "  def forward(self, xs):\n",
    "    xs = self.embed.forward(xs)\n",
    "    hs = self.lstm.forward(xs)\n",
    "    self.hs = hs\n",
    "    return self.hs\n",
    "  \n",
    "  def backward(self, dh):\n",
    "    dout = self.lstm.backward(dh)\n",
    "    dout = self.embed.backward(dout)\n",
    "    return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kOIN4WtB9or"
   },
   "source": [
    "## AttentionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRUs3bbkZDqM"
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "    rn = np.random.randn\n",
    "    self.params, self.grads = [], []\n",
    "    \n",
    "    # weight initialization\n",
    "    embed_W = (rn(V, D) / 100).astype('f')\n",
    "    lstm_Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
    "    lstm_Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
    "    lstm_b = (np.zeros(4*H)).astype('f')\n",
    "    affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "    affine_b = (np.zeros(V)).astype('f')\n",
    "    \n",
    "    # layers\n",
    "    self.embed = TimeEmbedding(embed_W)\n",
    "    self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "    self.attention = TimeAttention()\n",
    "    self.affine = TimeAffine(affine_W, affine_b)\n",
    "    \n",
    "    layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "    for layer in layers:\n",
    "      self.params += layer.params\n",
    "      self.grads += layer.grads\n",
    "    \n",
    "    \n",
    "  def forward(self, xs, enc_hs):\n",
    "    embed = self.embed.forward(xs)\n",
    "    \n",
    "    h = enc_hs[:,-1]\n",
    "    self.lstm.set_state(h)\n",
    "    dec_hs = self.lstm.forward(embed)\n",
    "    \n",
    "    c = self.attention.forward(enc_hs, dec_hs)\n",
    "    out = np.concatenate((c, dec_hs), axis=2)\n",
    "    score = self.affine.forward(out)\n",
    "    return score\n",
    "  \n",
    "  def backward(self, dscore):\n",
    "    dout = self.affine.backward(dscore)\n",
    "    N, T, H2 = dout.shape\n",
    "    H = H2 // 2\n",
    "    \n",
    "    dc, dhs_dec0 = dout[:,:,:H], dout[:,:,H:]\n",
    "    dhs_enc, dhs_dec1 = self.attention.backward(dc)\n",
    "    \n",
    "    dhs_dec = dhs_dec0 + dhs_dec1\n",
    "    dxs = self.lstm.backward(dhs_dec)\n",
    "    dh = self.lstm.dh\n",
    "    \n",
    "    dhs_enc[:, -1] += dh\n",
    "    self.embed.backward(dxs)\n",
    "    \n",
    "    return dhs_enc\n",
    "  \n",
    "  def generate(self, h, start_id, sample_size):\n",
    "    sampled = []\n",
    "    sample_id = start_id\n",
    "    \n",
    "    _h = h[:,-1]\n",
    "    self.lstm.set_state(_h)\n",
    "    \n",
    "    for _ in range(sample_size):\n",
    "      x = np.array([sample_id]).reshape(1,1)\n",
    "      \n",
    "      embed = self.embed.forward(x)\n",
    "      hs_dec = self.lstm.forward(embed)\n",
    "      c = self.attention.forward(h, hs_dec)\n",
    "      out = np.concatenate((c, hs_dec), axis=2)\n",
    "      score = self.affine.forward(out)\n",
    "      \n",
    "      sample_id = np.argmax(score.flatten())\n",
    "      sampled.append(sample_id)\n",
    "      \n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HIAoB3Z3CAJp"
   },
   "source": [
    "## AttentionSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfppJnI6BCMi"
   },
   "outputs": [],
   "source": [
    "class AttentionSeq2Seq(Seq2seq):\n",
    "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "    args = vocab_size, wordvec_size, hidden_size\n",
    "    \n",
    "    # create layers\n",
    "    self.encoder = AttentionEncoder(*args)\n",
    "    self.decoder = AttentionDecoder(*args)\n",
    "    self.softmax = TimeSoftmaxWithLoss()\n",
    "    \n",
    "    self.params = self.encoder.params + self.decoder.params\n",
    "    self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCZjcX3wEHHC"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kP12DMYkCs_t"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data(\"./dataset/date.txt\")\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# reverse the input sequence\n",
    "x_train, x_test = x_train[:,::-1], x_test[:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1464,
     "status": "ok",
     "timestamp": 1557753586011,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "hniPz7aDFVef",
    "outputId": "7eff1a32-5df7-48d8-9d94-1165f3b4ed26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s': 0, 'e': 1, 'p': 2, 't': 3, 'm': 4, 'b': 5, 'r': 6, ' ': 7, '2': 8, '7': 9, ',': 10, '1': 11, '9': 12, '4': 13, '_': 14, '-': 15, '0': 16, 'A': 17, 'u': 18, 'g': 19, '3': 20, '8': 21, '/': 22, 'T': 23, 'U': 24, 'E': 25, 'S': 26, 'D': 27, 'Y': 28, 'P': 29, 'M': 30, 'B': 31, 'R': 32, '5': 33, 'J': 34, 'N': 35, '6': 36, 'a': 37, 'i': 38, 'l': 39, 'O': 40, 'c': 41, 'o': 42, 'G': 43, 'F': 44, 'y': 45, 'n': 46, 'C': 47, 'W': 48, 'd': 49, 'I': 50, 'L': 51, 'j': 52, 'H': 53, 'v': 54, 'h': 55, 'V': 56, 'f': 57, 'w': 58}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HauRR2fBFXBJ"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "vocab_size = len(id_to_char)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJ1rDQO_F9Cj"
   },
   "outputs": [],
   "source": [
    "model = AttentionSeq2Seq(vocab_size, wordvec_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPMud7t1nRl-"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam()\n",
    "trainer = Trainer(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10745
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3092962,
     "status": "ok",
     "timestamp": 1557737121394,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "9FN1_YxHGLRZ",
    "outputId": "39d5dc6e-068f-40b8-8d1f-054025343c5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
      "| epoch 1 |  iter 21 / 351 | time 13[s] | loss 3.09\n",
      "| epoch 1 |  iter 41 / 351 | time 26[s] | loss 1.90\n",
      "| epoch 1 |  iter 61 / 351 | time 39[s] | loss 1.72\n",
      "| epoch 1 |  iter 81 / 351 | time 51[s] | loss 1.46\n",
      "| epoch 1 |  iter 101 / 351 | time 64[s] | loss 1.19\n",
      "| epoch 1 |  iter 121 / 351 | time 77[s] | loss 1.14\n",
      "| epoch 1 |  iter 141 / 351 | time 90[s] | loss 1.09\n",
      "| epoch 1 |  iter 161 / 351 | time 104[s] | loss 1.06\n",
      "| epoch 1 |  iter 181 / 351 | time 118[s] | loss 1.04\n",
      "| epoch 1 |  iter 201 / 351 | time 130[s] | loss 1.03\n",
      "| epoch 1 |  iter 221 / 351 | time 143[s] | loss 1.02\n",
      "| epoch 1 |  iter 241 / 351 | time 155[s] | loss 1.02\n",
      "| epoch 1 |  iter 261 / 351 | time 168[s] | loss 1.01\n",
      "| epoch 1 |  iter 281 / 351 | time 182[s] | loss 1.00\n",
      "| epoch 1 |  iter 301 / 351 | time 194[s] | loss 1.00\n",
      "| epoch 1 |  iter 321 / 351 | time 206[s] | loss 1.00\n",
      "| epoch 1 |  iter 341 / 351 | time 219[s] | loss 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "accuracy: 0.000000\n",
      "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.00\n",
      "| epoch 2 |  iter 21 / 351 | time 13[s] | loss 1.00\n",
      "| epoch 2 |  iter 41 / 351 | time 26[s] | loss 0.99\n",
      "| epoch 2 |  iter 61 / 351 | time 39[s] | loss 0.99\n",
      "| epoch 2 |  iter 81 / 351 | time 52[s] | loss 0.99\n",
      "| epoch 2 |  iter 101 / 351 | time 65[s] | loss 0.99\n",
      "| epoch 2 |  iter 121 / 351 | time 77[s] | loss 0.99\n",
      "| epoch 2 |  iter 141 / 351 | time 90[s] | loss 0.98\n",
      "| epoch 2 |  iter 161 / 351 | time 102[s] | loss 0.98\n",
      "| epoch 2 |  iter 181 / 351 | time 117[s] | loss 0.97\n",
      "| epoch 2 |  iter 201 / 351 | time 129[s] | loss 0.95\n",
      "| epoch 2 |  iter 221 / 351 | time 142[s] | loss 0.94\n",
      "| epoch 2 |  iter 241 / 351 | time 154[s] | loss 0.90\n",
      "| epoch 2 |  iter 261 / 351 | time 166[s] | loss 0.83\n",
      "| epoch 2 |  iter 281 / 351 | time 179[s] | loss 0.74\n",
      "| epoch 2 |  iter 301 / 351 | time 193[s] | loss 0.66\n",
      "| epoch 2 |  iter 321 / 351 | time 206[s] | loss 0.58\n",
      "| epoch 2 |  iter 341 / 351 | time 218[s] | loss 0.46\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 2016-11-08\n",
      "---\n",
      "accuracy: 0.516800\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.35\n",
      "| epoch 3 |  iter 21 / 351 | time 13[s] | loss 0.30\n",
      "| epoch 3 |  iter 41 / 351 | time 25[s] | loss 0.21\n",
      "| epoch 3 |  iter 61 / 351 | time 38[s] | loss 0.14\n",
      "| epoch 3 |  iter 81 / 351 | time 52[s] | loss 0.09\n",
      "| epoch 3 |  iter 101 / 351 | time 65[s] | loss 0.07\n",
      "| epoch 3 |  iter 121 / 351 | time 78[s] | loss 0.05\n",
      "| epoch 3 |  iter 141 / 351 | time 90[s] | loss 0.04\n",
      "| epoch 3 |  iter 161 / 351 | time 103[s] | loss 0.03\n",
      "| epoch 3 |  iter 181 / 351 | time 117[s] | loss 0.03\n",
      "| epoch 3 |  iter 201 / 351 | time 131[s] | loss 0.02\n",
      "| epoch 3 |  iter 221 / 351 | time 143[s] | loss 0.02\n",
      "| epoch 3 |  iter 241 / 351 | time 156[s] | loss 0.02\n",
      "| epoch 3 |  iter 261 / 351 | time 168[s] | loss 0.01\n",
      "| epoch 3 |  iter 281 / 351 | time 181[s] | loss 0.01\n",
      "| epoch 3 |  iter 301 / 351 | time 193[s] | loss 0.01\n",
      "| epoch 3 |  iter 321 / 351 | time 207[s] | loss 0.01\n",
      "| epoch 3 |  iter 341 / 351 | time 220[s] | loss 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "accuracy: 0.999000\n",
      "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 0.01\n",
      "| epoch 4 |  iter 21 / 351 | time 13[s] | loss 0.01\n",
      "| epoch 4 |  iter 41 / 351 | time 25[s] | loss 0.01\n",
      "| epoch 4 |  iter 61 / 351 | time 38[s] | loss 0.01\n",
      "| epoch 4 |  iter 81 / 351 | time 51[s] | loss 0.01\n",
      "| epoch 4 |  iter 101 / 351 | time 65[s] | loss 0.01\n",
      "| epoch 4 |  iter 121 / 351 | time 77[s] | loss 0.00\n",
      "| epoch 4 |  iter 141 / 351 | time 90[s] | loss 0.01\n",
      "| epoch 4 |  iter 161 / 351 | time 102[s] | loss 0.00\n",
      "| epoch 4 |  iter 181 / 351 | time 116[s] | loss 0.00\n",
      "| epoch 4 |  iter 201 / 351 | time 129[s] | loss 0.00\n",
      "| epoch 4 |  iter 221 / 351 | time 143[s] | loss 0.00\n",
      "| epoch 4 |  iter 241 / 351 | time 155[s] | loss 0.00\n",
      "| epoch 4 |  iter 261 / 351 | time 168[s] | loss 0.00\n",
      "| epoch 4 |  iter 281 / 351 | time 180[s] | loss 0.00\n",
      "| epoch 4 |  iter 301 / 351 | time 193[s] | loss 0.00\n",
      "| epoch 4 |  iter 321 / 351 | time 205[s] | loss 0.00\n",
      "| epoch 4 |  iter 341 / 351 | time 220[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "accuracy: 0.999000\n",
      "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 5 |  iter 21 / 351 | time 13[s] | loss 0.00\n",
      "| epoch 5 |  iter 41 / 351 | time 25[s] | loss 0.00\n",
      "| epoch 5 |  iter 61 / 351 | time 38[s] | loss 0.00\n",
      "| epoch 5 |  iter 81 / 351 | time 50[s] | loss 0.00\n",
      "| epoch 5 |  iter 101 / 351 | time 64[s] | loss 0.00\n",
      "| epoch 5 |  iter 121 / 351 | time 78[s] | loss 0.00\n",
      "| epoch 5 |  iter 141 / 351 | time 90[s] | loss 0.00\n",
      "| epoch 5 |  iter 161 / 351 | time 103[s] | loss 0.00\n",
      "| epoch 5 |  iter 181 / 351 | time 116[s] | loss 0.00\n",
      "| epoch 5 |  iter 201 / 351 | time 130[s] | loss 0.00\n",
      "| epoch 5 |  iter 221 / 351 | time 142[s] | loss 0.00\n",
      "| epoch 5 |  iter 241 / 351 | time 156[s] | loss 0.00\n",
      "| epoch 5 |  iter 261 / 351 | time 169[s] | loss 0.00\n",
      "| epoch 5 |  iter 281 / 351 | time 181[s] | loss 0.00\n",
      "| epoch 5 |  iter 301 / 351 | time 194[s] | loss 0.00\n",
      "| epoch 5 |  iter 321 / 351 | time 206[s] | loss 0.00\n",
      "| epoch 5 |  iter 341 / 351 | time 219[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "accuracy: 0.999200\n",
      "| epoch 6 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 6 |  iter 21 / 351 | time 13[s] | loss 0.00\n",
      "| epoch 6 |  iter 41 / 351 | time 26[s] | loss 0.00\n",
      "| epoch 6 |  iter 61 / 351 | time 38[s] | loss 0.00\n",
      "| epoch 6 |  iter 81 / 351 | time 52[s] | loss 0.00\n",
      "| epoch 6 |  iter 101 / 351 | time 64[s] | loss 0.00\n",
      "| epoch 6 |  iter 121 / 351 | time 78[s] | loss 0.00\n",
      "| epoch 6 |  iter 141 / 351 | time 91[s] | loss 0.00\n",
      "| epoch 6 |  iter 161 / 351 | time 104[s] | loss 0.00\n",
      "| epoch 6 |  iter 181 / 351 | time 117[s] | loss 0.00\n",
      "| epoch 6 |  iter 201 / 351 | time 130[s] | loss 0.00\n",
      "| epoch 6 |  iter 221 / 351 | time 143[s] | loss 0.00\n",
      "| epoch 6 |  iter 241 / 351 | time 156[s] | loss 0.00\n",
      "| epoch 6 |  iter 261 / 351 | time 170[s] | loss 0.00\n",
      "| epoch 6 |  iter 281 / 351 | time 182[s] | loss 0.00\n",
      "| epoch 6 |  iter 301 / 351 | time 195[s] | loss 0.00\n",
      "| epoch 6 |  iter 321 / 351 | time 208[s] | loss 0.00\n",
      "| epoch 6 |  iter 341 / 351 | time 220[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1994-05-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 2013-08-22\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 2016-01-06\n",
      "---\n",
      "accuracy: 0.810400\n",
      "| epoch 7 |  iter 1 / 351 | time 0[s] | loss 0.11\n",
      "| epoch 7 |  iter 21 / 351 | time 14[s] | loss 0.03\n",
      "| epoch 7 |  iter 41 / 351 | time 27[s] | loss 0.01\n",
      "| epoch 7 |  iter 61 / 351 | time 39[s] | loss 0.00\n",
      "| epoch 7 |  iter 81 / 351 | time 53[s] | loss 0.00\n",
      "| epoch 7 |  iter 101 / 351 | time 65[s] | loss 0.00\n",
      "| epoch 7 |  iter 121 / 351 | time 78[s] | loss 0.00\n",
      "| epoch 7 |  iter 141 / 351 | time 92[s] | loss 0.00\n",
      "| epoch 7 |  iter 161 / 351 | time 105[s] | loss 0.00\n",
      "| epoch 7 |  iter 181 / 351 | time 118[s] | loss 0.00\n",
      "| epoch 7 |  iter 201 / 351 | time 132[s] | loss 0.00\n",
      "| epoch 7 |  iter 221 / 351 | time 144[s] | loss 0.00\n",
      "| epoch 7 |  iter 241 / 351 | time 157[s] | loss 0.00\n",
      "| epoch 7 |  iter 261 / 351 | time 170[s] | loss 0.00\n",
      "| epoch 7 |  iter 281 / 351 | time 183[s] | loss 0.00\n",
      "| epoch 7 |  iter 301 / 351 | time 196[s] | loss 0.00\n",
      "| epoch 7 |  iter 321 / 351 | time 208[s] | loss 0.00\n",
      "| epoch 7 |  iter 341 / 351 | time 221[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "accuracy: 1.000000\n",
      "| epoch 8 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 8 |  iter 21 / 351 | time 13[s] | loss 0.00\n",
      "| epoch 8 |  iter 41 / 351 | time 27[s] | loss 0.00\n",
      "| epoch 8 |  iter 61 / 351 | time 40[s] | loss 0.00\n",
      "| epoch 8 |  iter 81 / 351 | time 53[s] | loss 0.00\n",
      "| epoch 8 |  iter 101 / 351 | time 65[s] | loss 0.00\n",
      "| epoch 8 |  iter 121 / 351 | time 78[s] | loss 0.00\n",
      "| epoch 8 |  iter 141 / 351 | time 91[s] | loss 0.00\n",
      "| epoch 8 |  iter 161 / 351 | time 104[s] | loss 0.00\n",
      "| epoch 8 |  iter 181 / 351 | time 118[s] | loss 0.00\n",
      "| epoch 8 |  iter 201 / 351 | time 132[s] | loss 0.00\n",
      "| epoch 8 |  iter 221 / 351 | time 144[s] | loss 0.00\n",
      "| epoch 8 |  iter 241 / 351 | time 157[s] | loss 0.00\n",
      "| epoch 8 |  iter 261 / 351 | time 169[s] | loss 0.00\n",
      "| epoch 8 |  iter 281 / 351 | time 183[s] | loss 0.00\n",
      "| epoch 8 |  iter 301 / 351 | time 196[s] | loss 0.00\n",
      "| epoch 8 |  iter 321 / 351 | time 209[s] | loss 0.00\n",
      "| epoch 8 |  iter 341 / 351 | time 221[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "accuracy: 1.000000\n",
      "| epoch 9 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 9 |  iter 21 / 351 | time 13[s] | loss 0.00\n",
      "| epoch 9 |  iter 41 / 351 | time 25[s] | loss 0.00\n",
      "| epoch 9 |  iter 61 / 351 | time 39[s] | loss 0.00\n",
      "| epoch 9 |  iter 81 / 351 | time 52[s] | loss 0.00\n",
      "| epoch 9 |  iter 101 / 351 | time 65[s] | loss 0.00\n",
      "| epoch 9 |  iter 121 / 351 | time 77[s] | loss 0.00\n",
      "| epoch 9 |  iter 141 / 351 | time 90[s] | loss 0.00\n",
      "| epoch 9 |  iter 161 / 351 | time 102[s] | loss 0.00\n",
      "| epoch 9 |  iter 181 / 351 | time 116[s] | loss 0.00\n",
      "| epoch 9 |  iter 201 / 351 | time 130[s] | loss 0.00\n",
      "| epoch 9 |  iter 221 / 351 | time 142[s] | loss 0.00\n",
      "| epoch 9 |  iter 241 / 351 | time 155[s] | loss 0.00\n",
      "| epoch 9 |  iter 261 / 351 | time 167[s] | loss 0.00\n",
      "| epoch 9 |  iter 281 / 351 | time 180[s] | loss 0.00\n",
      "| epoch 9 |  iter 301 / 351 | time 193[s] | loss 0.00\n",
      "| epoch 9 |  iter 321 / 351 | time 207[s] | loss 0.00\n",
      "| epoch 9 |  iter 341 / 351 | time 219[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "accuracy: 1.000000\n",
      "| epoch 10 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 10 |  iter 21 / 351 | time 13[s] | loss 0.00\n",
      "| epoch 10 |  iter 41 / 351 | time 25[s] | loss 0.00\n",
      "| epoch 10 |  iter 61 / 351 | time 39[s] | loss 0.00\n",
      "| epoch 10 |  iter 81 / 351 | time 53[s] | loss 0.00\n",
      "| epoch 10 |  iter 101 / 351 | time 65[s] | loss 0.00\n",
      "| epoch 10 |  iter 121 / 351 | time 78[s] | loss 0.00\n",
      "| epoch 10 |  iter 141 / 351 | time 90[s] | loss 0.00\n",
      "| epoch 10 |  iter 161 / 351 | time 103[s] | loss 0.00\n",
      "| epoch 10 |  iter 181 / 351 | time 115[s] | loss 0.00\n",
      "| epoch 10 |  iter 201 / 351 | time 131[s] | loss 0.00\n",
      "| epoch 10 |  iter 221 / 351 | time 143[s] | loss 0.00\n",
      "| epoch 10 |  iter 241 / 351 | time 156[s] | loss 0.00\n",
      "| epoch 10 |  iter 261 / 351 | time 168[s] | loss 0.00\n",
      "| epoch 10 |  iter 281 / 351 | time 181[s] | loss 0.00\n",
      "| epoch 10 |  iter 301 / 351 | time 193[s] | loss 0.00\n",
      "| epoch 10 |  iter 321 / 351 | time 206[s] | loss 0.00\n",
      "| epoch 10 |  iter 341 / 351 | time 220[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "acc_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "  trainer.fit(x_train, t_train, max_epoch=1, \n",
    "              batch_size=batch_size, max_grad=max_grad)\n",
    "  correct_num = 0\n",
    "  for i in range(len(x_test)):\n",
    "    question, correct = x_test[[i]], t_test[[i]]\n",
    "    verbose = i < 10\n",
    "    correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose, is_reverse=True)\n",
    "    \n",
    "  acc = float(correct_num) / len(x_test)\n",
    "  acc_list.append(acc)\n",
    "  print(\"accuracy: {:3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 848,
     "status": "ok",
     "timestamp": 1557740979960,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "q7Oz7ry8HEEW",
    "outputId": "19c30ac6-5a77-4614-9ae7-9c5abb71301a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUVEQEZEmKgoiItLk/wMBqg+1AeGtwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_list)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accurcy (0-100%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wiCQCe2lmQM7"
   },
   "outputs": [],
   "source": [
    "model.save_params(\"./tmp/attentionseq2seq.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cZbxnjdnAWY"
   },
   "outputs": [],
   "source": [
    "model.load_params(\"./tmp/attentionseq2seq.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1BsHNM7GWlRm"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 856,
     "status": "ok",
     "timestamp": 1557754297276,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "D7KZFAXeWfDE",
    "outputId": "13896c91-2246-479a-f1f3-afe74bd93099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  7  7  7  7  7  7  7  8 12 12 11  7 10 13 11  7  1 46 18 34  7 10 45\n",
      " 37 49 46 18 26]\n",
      "        2991 ,41 enuJ ,yadnuS\n"
     ]
    }
   ],
   "source": [
    "test_id = 10\n",
    "print(x_test[test_id])\n",
    "print(''.join([id_to_char[w] for w in x_test[test_id]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 847,
     "status": "ok",
     "timestamp": 1557754325102,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "zmWmgB1GYCSB",
    "outputId": "7d216cb8-4efd-495a-cb1e-dbcfafa4c2a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  7  7  7  7  7  7  7  8 12 12 11  7 10 13 11  7  1 46 18 34  7 10 45\n",
      "  37 49 46 18 26]]\n",
      "26\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "xs = np.array([x_test[test_id]]).reshape(1, -1)\n",
    "start_id = xs[0,-1]\n",
    "sample_size = len(t_test[test_id])-1\n",
    "\n",
    "print(xs)\n",
    "print(start_id)\n",
    "print(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1557754326662,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "fLzmq6CcXLFQ",
    "outputId": "09e0a97c-6822-4020-bb59-c53ea3f74d9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992-06-14\n"
     ]
    }
   ],
   "source": [
    "date_res = model.generate(xs, start_id, sample_size)\n",
    "print(''.join([id_to_char[w] for w in date_res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1557754702808,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "lCkuxmTtZ85-",
    "outputId": "8008e4c6-2705-4244-fde8-cfbf84dbf3bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 29)"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_weight = model.decoder.attention.attention_weights[0]\n",
    "time_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1557754789875,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "BpxQq51aZtFQ",
    "outputId": "ad38e231-d88f-40d1-8189-2d1cdf511714"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.6678264e-14, 2.5502697e-12, 1.3931033e-11, 6.0465626e-12,\n",
       "        2.3383819e-11, 4.5732848e-11, 2.1273146e-11, 1.1988932e-11,\n",
       "        1.8943877e-07, 1.2607745e-11, 1.9219035e-09, 9.9848023e-07,\n",
       "        5.4464457e-05, 3.6977945e-06, 9.9993932e-01, 4.4847978e-10,\n",
       "        1.8685490e-08, 3.5948625e-10, 1.0686457e-06, 1.7316913e-09,\n",
       "        1.1563957e-11, 8.3078501e-12, 3.8256658e-11, 5.0872173e-10,\n",
       "        1.0847767e-10, 2.2193060e-10, 2.7224649e-15, 5.0866613e-11,\n",
       "        1.8521462e-07]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1Up0Fp1by1W"
   },
   "source": [
    "### Attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1894,
     "status": "ok",
     "timestamp": 1557756298804,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "UYX8inUqa7wY",
    "outputId": "d3a51a99-a104-4118-992a-b2ed03efb1da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "19\n",
      "199\n",
      "1992\n",
      "1992-\n",
      "1992-0\n",
      "1992-06\n",
      "1992-06-\n",
      "1992-06-1\n",
      "1992-06-14\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "test_id = 10\n",
    "xs = np.array([x_test[test_id]]).reshape(1, -1)\n",
    "start_id = xs[0,-1]\n",
    "sample_size = len(t_test[test_id])-1\n",
    "\n",
    "attention_weights = []\n",
    "for l in range(1, sample_size+1):\n",
    "  date_res = model.generate(xs, start_id, l)\n",
    "  print(''.join([id_to_char[w] for w in date_res]))\n",
    "  \n",
    "  time_weight = model.decoder.attention.attention_weights[0]\n",
    "  attention_weights.append(np.squeeze(time_weight))\n",
    "  \n",
    "print(len(attention_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdcilXlpedbj"
   },
   "outputs": [],
   "source": [
    "for weight in attention_weights:\n",
    "  weight[...] = weight[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2228,
     "status": "ok",
     "timestamp": 1557756299554,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "N8IY7IQLbb7_",
    "outputId": "90dc419b-f995-4546-87eb-a9a5f5e2d85b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 29)\n"
     ]
    }
   ],
   "source": [
    "matrix = np.array(attention_weights)\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2088,
     "status": "ok",
     "timestamp": 1557756299555,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "goSIQprEciwt",
    "outputId": "086556b5-3dcf-4168-80d1-e7b24ff6325d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '9', '9', '2', '-', '0', '6', '-', '1', '4']\n"
     ]
    }
   ],
   "source": [
    "ylist = [id_to_char[w] for w in date_res]\n",
    "print(ylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1841,
     "status": "ok",
     "timestamp": 1557756299556,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "WAf7OTOBcyYH",
    "outputId": "9a9e0e22-cf74-47cf-dd81-3cdbb32cb278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'u', 'n', 'd', 'a', 'y', ',', ' ', 'J', 'u', 'n', 'e', ' ', '1', '4', ',', ' ', '1', '9', '9', '2', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "xlist = [id_to_char[w] for w in x_test[test_id]]\n",
    "xlist.reverse()\n",
    "print(xlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1677,
     "status": "ok",
     "timestamp": 1557756299557,
     "user": {
      "displayName": "王DevOps",
      "photoUrl": "https://lh6.googleusercontent.com/-DzgWpvOQ_c0/AAAAAAAAAAI/AAAAAAAAA8I/fX9gQd_TAjU/s64/photo.jpg",
      "userId": "04300517850278510646"
     },
     "user_tz": -480
    },
    "id": "9BUO2IUmeIjS",
    "outputId": "8df1e6ef-3a97-4c43-da47-2000c18b16c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, '1'),\n",
       " Text(0, 0, '9'),\n",
       " Text(0, 0, '9'),\n",
       " Text(0, 0, '2'),\n",
       " Text(0, 0, '-'),\n",
       " Text(0, 0, '0'),\n",
       " Text(0, 0, '6'),\n",
       " Text(0, 0, '-'),\n",
       " Text(0, 0, '1'),\n",
       " Text(0, 0, '4')]"
      ]
     },
     "execution_count": 150,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAACWCAYAAAD+FGJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(matrix)\n",
    "ax.set_xticks(np.arange(len(xlist)))\n",
    "ax.set_yticks(np.arange(len(ylist)))\n",
    "ax.set_xticklabels(xlist)\n",
    "ax.set_yticklabels(ylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UdkHjPSjfNGj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "18_Attention.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
